---
title: Controlling Testing Environments
date: 2008-04-29 13:16:00
tags: deployment, test-environments, testing, 
---
<div style="text-align: justify;"><span style="font-weight: bold;"></span><span style="font-size:130%;">Why You Should Care?</span><br /><span style="font-weight: bold;">Testing environments are fundamental to successful testing.</span> The test environment is where testing occurs and without a controlled, regulated, stable testing environment you are undermining your entire testing foundation. Scary stuff!<br /><br />What do I mean by controlling a testing environment? I mean ensuring:<br /></div><ul style="text-align: justify;"><li>that you know that each environment has the correct code,</li><li>that the various integrating applications have compatible versions,</li><li>that the correct hardware and software configuration exists,</li><li>that the data is legitimate and in the right quantities, </li><li>access to the environment is restricted and,</li><li>security policies mimic production</li></ul><div style="text-align: justify;">All of above items combine to make a stable, controlled, test environment.<br /><br />Without proper management of testing environments whenever a defect is identified you have to:<br /></div><ol style="text-align: justify;"><li>identify the software build, </li><li>determine how long that build has been there, </li><li>determine if there is a later build available? </li><li>ensure that the date is valid?</li><li>review the hardware to ensure it matches production</li><li>review the additional software components to ensure it matches production</li></ol><div style="text-align: justify;"><br />Beyond environmental stability there are particular test scenarios that you can now perform. <span style="font-weight: bold;">You can engage in deployment testing</span>. Every release the software package is released into production. How often is this software deployment process tested?<br /><br />Other benefits are: when you receive a "bad" build you can un-install it and re-install the previous one until it gets fixed. Or, you can get two competing builds from the development team and compare them for performance. I am doing this one next week.<br /><br /><br /><span style="font-size:130%;">So how do we go about doing this?</span><br />The first step is to identify how many test environments you have / need. In summary, I like to see at least     the following:<br /></div><ul style="text-align: justify;"><li><span style="font-weight: bold;">Development</span> - one per developer, usually the development box but ideally should be a VM or similar that matches production architecture/operating system/software configuration. Developers may call it a build box, but they do unit testing here, so it is a test environment. </li><li><span style="font-weight: bold;">Development integration</span> - one per project/release. Here the development team works on integrating their individual components together. </li><li><span style="font-weight: bold;">Test</span> - where the brunt of the tester's work is done. There should be a dedicated environment for each project.</li></ul><div style="text-align: justify;">The following environments can usually be shared between project teams depending on the number and types of projects being developed concurrently.</div><ul style="text-align: justify;"><li><span style="font-weight: bold;">User acceptance testing </span>- can be done in other environments if the resources are not available. Ideally should be a dedicated environment that looks like prod + all code between now and project release. This is an optional environment in my opinion as there are lots of good places to do UAT and it really depends on the maturity of your project and your organisation's available infrastructure.</li><li><span style="font-weight: bold;">Non-functional </span>- performance, stress, load, robustness - should be identical infrastructure to production, the data requirements can exceed production quantities but must match it in authenticity. </li></ul><div style="text-align: justify;">More environments are possible. I didn't cover integration or release candidate environments (you may have duplicate environments or subsets for prod-1, prod and prod+1) and it really depends on the number of software products being developed concurrently. I won't be discussing the logistics of establishing test environments here nor how to acquire them cheaply.<br /><br />To actually gain control. First talk to the development team about your requirements for a stable testing environment. Explain your reasons and get their support. The next step is not always necessary but can give you good piece of mind. <span style="font-weight: bold;">Remove developer access to the test environments</span>. I am talking about everywhere, web servers, databases, terminal services, virtual machines. If its apart of the testing environment they should stay out.<br /><br />It isn't because you don't trust them. After deployment you probably shouldn't be on those machines either. Sure, there are some testing scenarios where getting into the nitty gritty is required, but not always and certainly not when testing from the user's perspective. The bottom line is that the less people who have access to these machines results in a smaller chance of accidental environmental comprise.<br /><br /><br /><span style="font-size:130%;">So what aspects do we control?</span><br />Primarily we need to <span style="font-weight: bold;">control the Entry and Exit criteria</span> to each environment. The first step is the development environment. Entry is entirely up to the developer and exit should be achieved when unit tests passed. As the next step is the development integration environment, the development lead should control code entry.<br /><br />Entry into the test environment: regardless of the development methodology the delivery to test should be scheduled. Development completes a build that delivers "N chunks" of functionality. Unit tests have passed and they are good to go.<br /><br /><span style="font-weight: bold;">Developers should then prepare a deployment package</span> (like they will for the eventual production release) and place it in a shared location that the deployment testers can access. It is now up to the deployment testers to deploy the code at the request of the project testing team (these are quite often the same team). Once a build has been deployed, some build verification tests are executed (preferably automated) and the testers can continue their work.<br /><br />To move from test into any environment afterwards (release candidate integration, pre-production, etc) depends on the organisation but usually the following: Testing has been completed, defects resolved, user documentation produced and <span style="font-weight: bold;">most importantly user sign-off has been acquired</span>.<br /><br />The final environments (pre-production, etc) are usually (should be) managed by a release manager who controls the entry and exit gates from each environment after test and on into production. I won't cover these here.<br /><br /><br /><span style="font-size:130%;">Evidence or it never happened! </span><br /><span style="font-weight: bold;">Example A:</span> About a month ago we had a problem where one of our test environments wasn't working as expected. It took the developer over a week to find the problem. Turns out another developer had promoted some code without letting anyone else know. The code didn't work and he left it there.<br /><br />This could have been avoided if the developer didn't have access to deploy in our environment. Unfortunately he does, but it is something that we are working towards rectifying.<br /><br /><span style="font-weight: bold;">Example B: </span>I once worked on a project that had five development teams. Two database groups and three code cutters. Had they been able to deploy when they wanted, our test environment would have been useless. None of the teams were ever ready at the same time and it would have meant we would have had code without appropriate database support. Components that were meant to integrate but did not match because the latest build of application x wasn't ready yet.<br /><br />By waiting until all builds were ready and running through the deployment ourselves we ensured that our test environment was stable and had the same level of development progression all the way through.<br /><span style="font-weight: bold;"></span><br /><br /><span style="font-size:130%;">Too much information, summarise before I stop caring!</span><br /></div><ol style="text-align: justify;"><li>Controlling Test Environments = Good</li><li>Focus on developing entry and exit criteria</li><li>Build up to production-like environments - each successive environment should be closer and closer to production.</li><li>Evolve towards the goal of environmental control rather than a big bang approach. Some transitions will take longer than others (i.e. getting the right hardware) so pick a level of control for each release, get everyone involved and implement it.</li><li>Get team buy in (developers, testers) - education is the key</li><li>Don't make the entry into the test environment documentation heavy.</li></ol><div style="text-align: justify;"><br /><span style="font-size:130%;">It all looks too easy, how could this go wrong?</span><br /><span style="font-weight: bold;">Get development buy-in</span>. This is important you don't want to alienate the development team. Not all developers or development teams are inconsiderate, nor do they have ulterior motives. Usually it's a simple lack of awareness and discussing with them the direction you want to take with the testing environments will achieve two things. Firstly, they have greater visibility into the testing arena and secondly they often realise that they can help <span style="font-weight: bold;">improve quality by doing less. </span>Who doesn't like doing that?<br /><br /><br /><span style="font-weight: bold;">Don't make it complicated:</span> The goal of this is to achieve a high quality test environment to facilitate high quality testing. Don't produce a set of forms and a series of hoops that you need to force various developers and teams to fill out whilst jumping through. They won't like it and they probably won't like you.<br /><br />When I first tried locking down an environment, I asked the developers to fill out a handover to test document that listed the build, implemented task items, resolved defects and similar items. I had buy in and for the first few cycles it worked ok. It wasn't great though. All I was doing was accumulating bits of paper and wasting their time by making them fill it out.<br /><br />All I do these days is <span style="font-weight: bold;">discuss with the developers</span> the reasons why the environment needs to be locked down and to let me know when a new build is ready. I'm usually involved in iteration planning meetings so I know what is coming anyway. All that waffle they had to fill out is automatically generated from defect management, task management and source control software.<br /><br />My testing environments are generally stable, developers are happy to hand me deployment packages and consider deployment defects just as important as normal defects. After all, <span style="font-weight: bold;">deployment is the first chance a piece of software has to fail in production</span>. It is also the first place user's will see your application.<br /><br />It takes time to move towards a controlled environment and as you read in my examples, my employer is not there yet either, but we are getting closer.<br /><br /><br /><span style="font-weight: bold;">One other note: </span>You may not have the ability (whether technical or organisational) to perform development testing. See if you can organise to sit with the technical team that does deployments for you.<br /></div>